# Semester timetable

Below are assignment due dates and a plan or record of the topics we covered each week this semester, updated after each week. See syllabus for overview of planned topics.




## Assignment due dates
| Assignment                       | Due             |
| -------------------------------- | --------------- |
| 1 (LOOCV)                        | Tue 27 Jan (Week 4) |
| 2 (classification, KNN)          | The 5 Feb (Week 5)  |
| 3 (decision trees, bagging)      | TBA (Week 7)    |
| 4 (boosting)                     | TBA (Week 9)    |
| 5 (neural networks)              | TBA (Week 12)   |
| Individual project presentations | TBA finals week |
| Individual project code or paper | TBA finals week |



## Discussions

| Lead | Week | Day  | Date.... | Paper |
| ---- | ---- | ---- | -------- | ----- |
| TBA  | 10   | Tue  | TBA      | TBA.  |
|      |      |      |          |       |
|      |      |      |          |       |
|      |      |      |          |       |
|      |      |      |          |       |
|      |      |      |          |       |



## Week 1

* Intro to class
* What is machine learning?
* Algorithms in data science: model, training, inference



## Week 2

* Statistical inference: accuracy of a trained model
* Regression and classification
* Polynomial model algorithm
* Optimization training algorithms
  * minimizing the training error
* Cross-validation (CV) inference algorithm
  * k-fold CV for regression
  * train-test split
  * mean squared error
* Tuning parameters



## Week 3

* Model algorithms
  * Smoothing splines
  * k nearest neighbors (KNN), regression and classification
* Training algorithm
  * regularization: penalized least squares
* CV inference algorithm
  * k-fold CV for classification
  * error rate
* Theory: bias-variance tradeoff



## Week 4

* Model algorithms
  * Decision tree models
  * Ensemble algorithms: bagging
* Training algorithms
  * Recursive binary partitioning for decision trees
* Inference algorithms
  * Tuning decision trees with CV



## Week 5

* Model algorithms
  * Ensemble algorithms: bagging, random forests
* Inference algorithms
  * Tuning bagging with CV
  * Tuning random forests with "out of bag" CV
  * Explainable machine learning: variable importance
* Parallel processing
* Using a compute server



## Week 6

* Model algorithms
  * Ensemble algorithm: boosted trees
* Training algorithms
  * Gradient descent
  * Stochastic gradient descent
  * Gradient boosting
  * Stochastic gradient boosting
  * Extreme gradient boosting, XGBoost
* Inference algorithms
  * Tuning strategies for multiple hyperparameters
* One hot encoding for categorical variables



## Week 7

* Model algorithms
  * Single layer neural networks
  * Architectures for regression, classification, multifunction
  * Deep learning: multilayer neural networks
  * Wide vs deep, expressiveness
* Training algorithm
  * Mini-batch stochastic gradient descent
  * Using Keras library



## Week 8

* Model algorithms
  * Deep learning: convolutional neural networks
* Training algorithms
  * Dropout regularization
  * Train-validate-test split
* Generalization & importance of data
  * Scope of inference
  * Test set leakage
  * Transfer learning, pre-trained models



## Week 9

* Model algorithms
  * Deep learning: transformers and other architectures



## Weeks 10-15

* Reading and discussion: contemporary and emerging applications in ecology
* Individual project
